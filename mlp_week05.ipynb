{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c903dbb6",
   "metadata": {},
   "source": [
    "# Week 5 - Regularization\n",
    "\n",
    "**Student Name 1, Student Name 2**\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will be able to \n",
    "\n",
    ">* perform regulized regression in sklearn\n",
    ">* understand the role of tuning parameter(s)\n",
    ">* use cross-validation for model tuning and comparison.\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Baseline Model](#baseline)\n",
    "4. [Ridge Regression](#ridge)\n",
    "4. [Lasso Regression](#lasso)\n",
    "4. [ElasticNet Regression](#elasticnet)\n",
    "\n",
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. You should aim to switch roles between driver and navigator approximately every 15 minutes. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bcaad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem Definition and Setup<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1cf67",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, let's load some of the packages you wil need for this workshop (we will load others as we progress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8753fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a1f9b",
   "metadata": {},
   "source": [
    "## User Defined Helper Functions\n",
    "\n",
    "We will make use of the two helper functions that we used last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50439ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should residual plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    MSE = mean_squared_error(y, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    Rsqr = r2_score(y, y_hat)\n",
    "    \n",
    "    Metrics = (round(MSE, 4), round(RMSE, 4), round(Rsqr, 4))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Observed vs Fitted values\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y_hat', y='resid', data=res).set_title(\"Fitted values vs Residuals\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model (MSE, RMSE, Rsq) = \" + str(Metrics), fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    return MSE, RMSE, Rsqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ccddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(m, plot = False, feature_names = None, figsize = (5,5), figtitle = None, intercept = True):\n",
    "    \"\"\"Returns model coefficients in a data frame for a fitted linear model.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn LinearRegression model object or pipeline with LinearRegression as final step\n",
    "        plot: boolean value, should coefficients be plotted with error bars\n",
    "        feature_names: list of feature names to use in the plot \n",
    "        figsize: tuple defining figure size\n",
    "        figtitle: string defining figure title\n",
    "        intercept: boolean value, should intercept be included in the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract intercept and coefficients into a single array\n",
    "    w = np.concatenate(([m[-1].intercept_] if isinstance(m, sklearn.pipeline.Pipeline) else [m.intercept_], \n",
    "                            m[-1].coef_ if isinstance(m, sklearn.pipeline.Pipeline) else m.coef_))\n",
    "    # Extract name of features\n",
    "    if feature_names is None:\n",
    "        feature_names = m[:-1].get_feature_names_out() if isinstance(m, sklearn.pipeline.Pipeline) else m.feature_names_in_\n",
    "    feature_names = np.concatenate((['intercept'], feature_names))\n",
    "    # Create a data frame\n",
    "    w_df = pd.DataFrame({'feature': feature_names, 'coef': w}).sort_values (\"coef\", ascending=False)\n",
    "\n",
    "    if plot:\n",
    "        if not intercept:\n",
    "            w_df = w_df[w_df['feature'] != 'intercept']\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.barh(w_df['feature'], w_df['coef'])\n",
    "        plt.ylabel('Features')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.axvline(x=0, color=\".5\")\n",
    "        if figtitle is not None:\n",
    "            plt.title(figtitle)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    return  w_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e767e",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this week's workshop comes from the Elements of Statistical Learning textbook. The data originally come from a study by [Stamey et al. (1989)](https://www.sciencedirect.com/science/article/abs/pii/S002253471741175X)  in which they examined the relationship between the level of prostate-specific antigen (`psa`) and a number of clinical measures in men who were about to receive a prostatectomy. The variables are as follows,\n",
    "\n",
    "* `lpsa` - log of the level of prostate-specific antigen\n",
    "* `lcavol` - log cancer volume\n",
    "* `lweight` - log prostate weight\n",
    "* `age` - patient age\n",
    "* `lbph` - log of the amount of benign prostatic hyperplasia\n",
    "* `svi` - seminal vesicle invasion\n",
    "* `lcp` - log of capsular penetration\n",
    "* `gleason` - Gleason score\n",
    "* `pgg45` - percent of Gleason scores 4 or 5\n",
    "* `train` - test / train split used in ESL\n",
    "\n",
    "These data are available in `prostate.csv`, which is included in the workshop materials.\n",
    "\n",
    "Let's start by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prostate = pd.read_csv('prostate.csv')\n",
    "prostate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed7be5",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis<a id='eda'></a>\n",
    "\n",
    "Before modelling, we will start with EDA to gain an understanding of the data, through descriptive statistics and visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef5250",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "a) Examine the data structure, look at the descriptive statistics, and create a pairs plot. Do any of our variables appear to be categorical / ordinal rather than numeric?\n",
    "\n",
    "b) Are there any interesting patterns in these data? Which variable appears likely to have the strongest relationship with `lpsa`? Why do you think we are exploring the relationship between these variables and `lpsa` (log of psa) rather than just psa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d01b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb1ef5d",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1517493",
   "metadata": {},
   "source": [
    "## Train-Test Set <a id='gen'></a>\n",
    "\n",
    "For these data we have already been provided a column to indicate which values should be used for the training set and which for the test set. This is encoded by the values in the `train` column - we can use these columns to separate our data and generate our training data: `X_train` and `y_train` as well as our test data `X_test` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data frames\n",
    "train = prostate.query(\"train == 'T'\").drop('train', axis=1)\n",
    "test = prostate.query(\"train == 'F'\").drop('train', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a192a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = train.drop(['lpsa'], axis=1)\n",
    "y_train = train.lpsa\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "X_test = test.drop('lpsa', axis=1)\n",
    "y_test = test.lpsa\n",
    "\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069b6e8",
   "metadata": {},
   "source": [
    "Let's also fix the random seed to make this notebook's output identical at every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "rng = np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c72ce",
   "metadata": {},
   "source": [
    "# Baseline model<a id='baseline'></a>\n",
    "\n",
    "Our first task is to fit a baseline model which we will be able to use as a point of comparison for our subsequent models. A good candidate for this is a simple linear regression model that includes all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897922c",
   "metadata": {},
   "source": [
    "Using our helper function, we can extract the coefficients for the model, which correspond to the variables: `lcavol`, `lweight`, `age`, `lbph`, `svi`, `lcp`, `gleason`, and `pgg45` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df = get_coefs(lm, plot=True, figsize=(6,6), figtitle=\"Linear Regression\", intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c76146",
   "metadata": {},
   "source": [
    "These coefficients have the typical regression interpretation, e.g. for each unit increase in `lcavol` we expect `lpsa` to increase by 0.5765 on average. To evaluate the predictive properities of our model, we will use the `model_fit` helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e108ad",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 2 (CORE)\n",
    "\n",
    "Use the `model_fit` function to evaluate both the model fit on the training data and the predictions on the test data. \n",
    "\n",
    "- Based on these plots do you see anything in the fit or residual plot that is potentially concerning? \n",
    "- Do you expect the MSE on test data to be better or worse than the MSE on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060c8f2",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b72fa1",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "In subsequent sections we will be exploring the use of the Ridge and Lasso regression models which both penalize larger values of $\\mathbf{w}$. While not particularly bad, our baseline model had coefficients that ranged from the smallest at 0.0095 to the largest at 0.737 which is about a 78x difference in magnitude. This difference can be made even worse if we were to change the units of one of our features, e.g. changing a measurement in kg to grams would change that coefficient by 1000 which has no effect on the fit of our linear regression model (predictions and other coefficients would be unchanged) but would have a meaningful impact on the estimates given by a Ridge or Lasso regression model, since that coefficient would now dominate the penalty term.\n",
    "\n",
    "To deal with this issue, the standard approach is to standaridize all features. Additionally, the feature values can now be interpreted as the number of standard deviations each observation is away from that column's mean.\n",
    "Using `sklearn` we can perform this transformation using the `StandardScaler` transformer from the preprocessing submodule.\n",
    "\n",
    "Keep in mind, that in order to avoid **data leakage** and get a realistic idea of the performance of model on the test data, **the mean and standard deviation used to standardize both the training and test sets should be computed from the training data only**.  The best way to accomplish this is to include the StandardScaler in a modeling pipeline for your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe7457",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 3 (CORE)\n",
    "\n",
    "Consider the following pipeline that first standardizes the numeric features before linear regression. Fit the model to the training data.  Using this new model what has changed about our model results? Comment on both the model's coefficients as well as its predictive performance. How has the interpretation of coefficients changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea105199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression pipeline, including standardization\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Names of numeric features\n",
    "num_features = ['lcavol','lweight','age', 'lbph','lcp', 'pgg45']\n",
    "# Names of binary features\n",
    "bin_features = ['svi']\n",
    "ord_features = ['gleason']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), num_features),\n",
    "    (OrdinalEncoder(categories=[[6,7,8,9]]),ord_features),\n",
    "    ('passthrough', bin_features),\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "lm_s = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearRegression()\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98053c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d7dd8",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5fa82",
   "metadata": {},
   "source": [
    "# Ridge Regression<a id='ridge'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c8636",
   "metadata": {},
   "source": [
    "Ridge regression is a natural extension to linear regression which introduces an $\\ell_2$ penalty on the coefficients in a standard least squares problem. \n",
    "\n",
    "The [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) model is provided by the `linear_model` submodule. Note that the penalty parameter (referred to as $\\lambda$ in the lecture notes) is called `alpha` is sklearn, and, as discussed in lectures, this parameter crucially determines the amount of shrinkage towards zero and the weight of the $\\ell_2$ penalty.\n",
    "\n",
    "After defining the ridge regression model via, e.g. `Ridge(alpha = 1)`, the usual methods can be called, such as `.fit()` to fit the model and `.predict()` to make predictions. \n",
    "\n",
    "As for the `LinearRegression()`, after fitting, the intercept and coefficients are stored separately in the attributes `.intercept_` and `.coef_`. In Ridge, this is helpful as it highlights how the penalty is only applied to the coefficient (i.e. we do not want to shrink the intercept).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178f0d1",
   "metadata": {},
   "source": [
    "Let's start by fitting a ridge regression model with $\\alpha=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected alpha value \n",
    "alpha_val = 1\n",
    "\n",
    "# Ridge pipeline\n",
    "r = make_pipeline(\n",
    "    preprocessor,\n",
    "    Ridge(alpha = alpha_val)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "model_fit(r, X_test, y_test, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dr_r = get_coefs(r, plot=True, figsize=(6,6), figtitle=\"Ridge Regression (alpha=\"+str(alpha_val)+\")\", intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845b526",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE)\n",
    "\n",
    "Adjust the value of `alpha` in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the MSE change? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e745",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b03ee",
   "metadata": {},
   "source": [
    "\n",
    "## Solution path: Ridge coeffcients as a function of $\\alpha$\n",
    "\n",
    "A useful way of examining the behavior of Ridge regression models is to plot the **solution path** of the coefficents $\\mathbf{w}$ as a function of the penalty parameter $\\alpha$. Since Ridge regression is equivalent to linear regression when $\\alpha=0$, we can see that as we increase the value of $\\alpha$, we are shrinking all of the coefficients in $\\mathbf{w}$ towards zero asymptotically $\\alpha$ approaches infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of alpha values\n",
    "alphas = np.logspace(-2, 3, num=200) # from 10^-2 to 10^3\n",
    "\n",
    "ws = [] # Store coefficients\n",
    "mses_train = [] # Store training mses\n",
    "mses_test = [] # Store test mses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        preprocessor,\n",
    "        Ridge(alpha=a)\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    ws.append(m[-1].coef_) \n",
    "    mses_train.append(mean_squared_error(y_train, m.predict(X_train)))\n",
    "    mses_test.append(mean_squared_error(y_test, m.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame for plotting\n",
    "sol_path = pd.DataFrame(\n",
    "    data = ws,\n",
    "    columns = m[0].get_feature_names_out()\n",
    ").assign(\n",
    "    alpha = alphas,\n",
    ").melt(\n",
    "    id_vars = ('alpha')\n",
    ")\n",
    "\n",
    "# Plot solution path of the weights\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.lineplot(x='alpha', y='value', hue='variable', data=sol_path)\n",
    "ax.axhline(y=0, color = \"black\", linestyle='dashed')\n",
    "ax.set_title(\"Ridge Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494faeb",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `lpsa`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a7a75",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5fc5dc",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (CORE)\n",
    "\n",
    "Run the code below to also plot both the training and test MSE as a function of $\\alpha$. What do you notice about the MSE as we increase $\\alpha$? Which value of $\\alpha$ seems better regarding the changes on training and testing MSE values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of MSE as function of alpha\n",
    "mses_path = pd.DataFrame(\n",
    "    {'alpha': alphas, 'Train': np.asarray(mses_train), 'Test': np.asarray(mses_test)}).melt(\n",
    "    id_vars = ('alpha')\n",
    ")\n",
    "\n",
    "# Plot MSE path\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.lineplot(x='alpha', y='value', hue='variable', data=mses_path)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "# ax.set_xlim(0,200) # Optional: to zoom in on lower alpha values\n",
    "# To remove legend title\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2a5ca",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cb6d2",
   "metadata": {},
   "source": [
    "## Tuning the penalty parameter with cross-validation\n",
    "\n",
    "We see that the value of $\\alpha$ crucially determines the performance of the ridge regression model. While `RidgeRegression()` uses the default value of `alpha=1`, this **should never be used in practice**. Instead, this parameter can be **tuned using cross-validation**. \n",
    "\n",
    "As with the polynomial models from last week, we can use `GridSearchCV` to employ k-fold cross validation to determine an optimal $\\alpha$. Remember, you can use the method `.get_params()` on your pipeline to list the parameters names to specify in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of tuning parameters\n",
    "alphas = np.linspace(0, 30, num=201)  \n",
    "\n",
    "#Pipeline\n",
    "m = make_pipeline(\n",
    "        preprocessor,\n",
    "        Ridge())\n",
    "# To get the parameter name for grid search\n",
    "# m.get_params()\n",
    "\n",
    "# CV strategy\n",
    "cv = KFold(5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Grid search\n",
    "gs = GridSearchCV(m,\n",
    "    param_grid={'ridge__alpha': alphas},\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\")\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefa643",
   "metadata": {},
   "source": [
    "Note that we are passing `sklearn.model_selection.KFold(5, shuffle=True, random_state=1234)` to the `cv` argument rather than leaving it to its default. This is because, while not obvious, the prostate data is structured (sorted by `lpsa` value) and this way we are able to ensure that the folds are properly shuffled. Failing to do this causes *very* unreliable results from the cross validation process.\n",
    "\n",
    "Once fit, we can examine the results to determine what value of $\\alpha$ was chosen as well as examine the results of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b24f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(gs.best_estimator_, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455db1b1",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "- How does this model compare to the performance of our baseline model? Is it better or worse?\n",
    "\n",
    "- How do the model coefficients for this model compare to the baseline model? To answer this, create a scatter plot of the coefficients for the baseline model against the coefficients for the ridge model. Are they always higher or lower? Now, use `np.linalg.norm` to compute the $\\ell_2$ norm of the coeffcients for both models and comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41521b00",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed7db5",
   "metadata": {},
   "source": [
    "As we saw last week, it is also recommend to plot the CV scores. Although the grid search may report a best value for the parameter corresponding to the maximum CV score (e.g. min CV MSE), if the curve is relatively flat around the minimum, we prefer the simpler model. \n",
    "\n",
    "Recall from last week that we can access the cross-validated scores (along with other results for each split) in the attribute `cv_results_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e866da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(gs.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092ea07",
   "metadata": {},
   "source": [
    "In particular, let's examining the `mean_test_score` and the `split#_test_score` keys since these are used to determine the optimal $\\alpha$.\n",
    "\n",
    "In the code below we extract these data into a data frame by selecting our columns of interest along with the $\\alpha$ values used (and transform negative MSE values into positive values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only mean and split scores\n",
    "cv_mse = pd.DataFrame(\n",
    "    data = gs.cv_results_\n",
    ").filter(\n",
    "    # Extract the split#_test_score and mean_test_score columns\n",
    "    regex = '(split[0-9]+|mean)_test_score'\n",
    ").assign(\n",
    "    # Add the alphas as a column\n",
    "    alpha = alphas\n",
    ")\n",
    "\n",
    "cv_mse.update(\n",
    "    # Convert negative mses to positive\n",
    "    -1 * cv_mse.filter(regex = '_test_score')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e20e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV MSE\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.lineplot(x='alpha', y='mean_test_score', data=cv_mse)\n",
    "ax.set_ylabel('CV MSE')\n",
    "ax.axvline(x=gs.best_params_['ridge__alpha'], color='red', linestyle='dashed', label='Best alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3eff5",
   "metadata": {},
   "source": [
    "This plot shows that the value of $\\alpha=1.05$ corresponds to the minimum of this curve. However, this plot gives us an overly confident view of this particular value of $\\alpha$. Specifically, if instead of just plotting the mean MSE across all of the validation sets, we also examine the MSE for each fold individually and the corresponding optimal value of $\\alpha$, we see that there is a lot of noise in the MSE and we should take the value $\\alpha = 1.05$ with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e7800",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 8 (CORE)\n",
    "\n",
    "Run the code below to plot the MSE for each validation set in the 5-fold cross validation. Why do you think that our cross validation results are unstable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data frame for plotting\n",
    "d = cv_mse.melt(\n",
    "    id_vars=('alpha','mean_test_score'),\n",
    "    var_name='fold',\n",
    "    value_name='MSE'\n",
    ")\n",
    "\n",
    "# Plot the validation scores across folds\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.lineplot(x='alpha', y='MSE', color='black', errorbar=None, data = d)  # Plot the mean MSE in black.\n",
    "sns.lineplot(x='alpha', y='MSE', hue='fold', data = d) # Plot the curves for each fold in different colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5123f3",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b0f3b",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 9 (CORE)\n",
    "\n",
    "Lastly, try changing the random seed in the cross-validation scheme. Plot the CV MSE with the optimal value of alpha marked with a vertical dashed line. How do the results change? Are different values of `alpha` suggested? Comment on your preffered value of `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV strategy\n",
    "cv = KFold(5, shuffle=True, random_state=0)\n",
    "\n",
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99149448",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a270f7a",
   "metadata": {},
   "source": [
    "*Note:* Due to the importance of tuning the value of $\\alpha$ in ridge regression, sklearn provides a function called [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) which combines `Ridge` with `GridSearchCV`. However, we will avoid using this function for two reasons:\n",
    "\n",
    "- it does not allow us to account for additional steps in our pipeline such as standardization when carrying out cross validation, resulting in _data leakage_\n",
    "- it only allows storing all results of the cross-validation in the attribute `.cv_results_` in the case of the default leave-one-out cross validation, with option `store_cv_results=True`. So, if you want to access all results and use a cross-validation strategy other than leave-one-out, you will need to use `GridSearchCV`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b7ee2",
   "metadata": {},
   "source": [
    "# Lasso Regression<a id='lasso'></a>\n",
    "\n",
    "We saw that ridge regression with a wise choice of $\\alpha$ can outperform our baseline linear regression. We can now investigate if lasso can yield a more accurate or interpretable solution. Recall that lasso uses an $\\ell_1$ penalty on the coefficients, as opposed to the $\\ell_2$ penalty of ridge. \n",
    "\n",
    "The [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model is also provided by the `linear_model` submodule and similarly requires the choice of the tuning parameter `alpha` to determine the weight of the $\\ell_1$ penalty. \n",
    "\n",
    "Try running the code below with different values of $\\alpha$ to see how it effects sparsity in the coefficients and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Selected alpha value \n",
    "alpha_val = 0.15\n",
    "\n",
    "# Lasso pipeline\n",
    "l = make_pipeline(\n",
    "    preprocessor,\n",
    "    Lasso(alpha = alpha_val)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "model_fit(l, X_test, y_test, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dr_r = get_coefs(l, plot=True, figsize=(6,6), figtitle=\"Lass Regression (alpha=\"+str(alpha_val)+\")\", intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae5633",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 10 (CORE)\n",
    "\n",
    "a) Plot the solution path of the coefficients as a function of $\\alpha$.\n",
    "\n",
    "b) How does this differ between the solution path for Ridge for large $\\alpha$? for small $\\alpha$?\n",
    "\n",
    "c) Which variable seems to be the most important for predicting `lpsa`?\n",
    "\n",
    "*Note that $\\alpha = 0$ causes a warning due to the fitting method (coordinate descent) not converging well without regularization (the $\\ell_1$ penalty here). So, the grid of $\\alpha$ values needs to start at some small positive constant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721df3c",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d8ed8",
   "metadata": {},
   "source": [
    "## Tuning the Lasso penalty parameter\n",
    "\n",
    "Again, we can use the `GridSearchCV` function to tune our Lasso model and optimize the $\\alpha$ hyperparameter. You could also use [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), which combines `Lasso` and `GridSearchCV` but we will focus on the former to avoid _data leakage_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f2d8d",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "a) Use `GridSearchCV` to find the optimal value of $\\alpha$.  \n",
    "\n",
    "b) Plot the CV MSE and MSE for each fold. Comment on the stability and uncertainty of $\\alpha$ across the different folds. Try changing the value of the `random_state` in the CV strategy - do the results change? are they more or less stable compared with ridge?\n",
    "\n",
    "c) Which variables are included with this optimal value of $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84aecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eccfc",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ddd598",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 12 (CORE)\n",
    "\n",
    "Run the following code to compute the CV MSE for the linear model and compare with the CV MSE of the lasso model to suggest an optimal value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso doesn't allow for alpha=0, so compute CV MSE for linear regression model to compare with Lasso\n",
    "gs_l = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        preprocessor,\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    param_grid = {},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_mean_squared_error\"\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf6ee7",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8eaa6",
   "metadata": {},
   "source": [
    "# ElasticNet Regression<a id='elasticnet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc1c00",
   "metadata": {},
   "source": [
    "Lastly, we can use elastic net regression, which is hybrid between lasso and ridge, including both an $\\ell_1$ and $\\ell_2$ penalty. The [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) model is again provided by the `linear_model` submodule and minimizes the objective:\n",
    "$$ \\frac{1}{2N} || \\mathbf{y} - \\mathbf{X}\\mathbf{w} ||^2_2 + \\alpha \\rho ||\\mathbf{w}||_1\n",
    "+ 0.5 \\alpha (1 - \\rho) ||\\mathbf{w}||^2_2.$$\n",
    "\n",
    "In this parameterization, $\\rho$ determines relative strength of the $\\ell_1$ penalty compared to the $\\ell_2$ and is referred to as `l1_ratio` in `ElasticNet`. Thus, we can also fit ridge and lasso regression models with `ElasticNet` through appropriate choice of `l1_ratio`:\n",
    "- ridge corresponds to `l1_ratio=0`\n",
    "- lasso corresponds to `l1_ratio=1`\n",
    "\n",
    "The parameter $\\alpha$ is referred to as `alpha` in `ElasticNet` and controls the overall penalty relative the residual sum of squares. \n",
    "\n",
    "The general `ElasticNet` requires tuning of both `alpha` and `l1_ratio`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cee1b",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 13 (CORE)\n",
    "\n",
    "The following code plots the solution path for a specific value of `l1_ratio`. Try changing the value of `l1_ratio` (you may also want to change the maximal value of `alpha` for better visualization). How do the solution paths change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Grid of alpha values\n",
    "alphas = np.linspace(0.01, 1, num=200)\n",
    "# L1 ratio\n",
    "l = 1\n",
    "ws = [] # Store coefficients\n",
    "mses_train = [] # Store training mses\n",
    "mses_test = [] # Store test mses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        preprocessor,\n",
    "        ElasticNet(alpha=a, l1_ratio=l)\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    ws.append(m[-1].coef_) \n",
    "    mses_train.append(mean_squared_error(y_train, m.predict(X_train)))\n",
    "    mses_test.append(mean_squared_error(y_test, m.predict(X_test)))\n",
    "\n",
    "# Create a data frame with the solution path\n",
    "sol_path = pd.DataFrame(\n",
    "    data = ws,\n",
    "    columns = m[0].get_feature_names_out()\n",
    ").assign(\n",
    "    alpha = alphas,\n",
    ").melt(\n",
    "    id_vars = ('alpha')\n",
    ")\n",
    "\n",
    "# Plot the solution path of the weights\n",
    "plt.figure(figsize=[7,5])\n",
    "ax = sns.lineplot(x='alpha', y='value', hue='variable', data=sol_path)\n",
    "ax.set_title(f'Elastic Net with L1 ratio: {l}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80f443",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc0a97",
   "metadata": {},
   "source": [
    "## Tuning with Grid Search CV\n",
    "\n",
    "Again, we can use `GridSearchCV` (or [`ElasticNetCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)) to tune the parameters. In the following code, we use `GridSearchCV` to tune both `alpha` and `l1_ratio`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of tuning parameters\n",
    "alphas = np.linspace(0.01, 5, num=50)\n",
    "l1r = [0.01, .1, .5, .7, .9, .95, 1]\n",
    "\n",
    "# CV strategy\n",
    "cv = KFold(5, shuffle=True, random_state=1234)\n",
    "\n",
    "# Pipeline\n",
    "m = make_pipeline(\n",
    "        preprocessor,\n",
    "        ElasticNet())\n",
    "\n",
    "# Grid search\n",
    "gs_enet = GridSearchCV(m,\n",
    "                        param_grid={'elasticnet__alpha': alphas, 'elasticnet__l1_ratio': l1r},\n",
    "                        cv = cv,\n",
    "                        scoring=\"neg_mean_squared_error\",\n",
    "                        return_train_score=True)\n",
    "gs_enet.fit(X_train, y_train)\n",
    "\n",
    "gs_enet.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570fca1",
   "metadata": {},
   "source": [
    "Grid search returns the best value, but to better support these choices, let's plot the cv scores as a function of `alpha` for different values of `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean CV scores for each l1_ratio\n",
    "cv_results_df = pd.DataFrame(gs_enet.cv_results_)\n",
    "cv_results_df['alpha'] = cv_results_df['param_elasticnet__alpha']\n",
    "cv_results_df['l1_ratio'] = cv_results_df['param_elasticnet__l1_ratio']\n",
    "\n",
    "# Convert negative MSE to positive\n",
    "cv_results_df['mean_cv_mse'] = -cv_results_df['mean_test_score']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for l1_val in l1r:\n",
    "    subset = cv_results_df[cv_results_df['l1_ratio'] == l1_val]\n",
    "    plt.plot(subset['alpha'], subset['mean_cv_mse'], marker='o', label=f'L1 ratio={l1_val}')\n",
    "\n",
    "plt.axvline(x=gs_enet.best_params_['elasticnet__alpha'], color='red', linestyle='dashed', \n",
    "            label=f'Best alpha={gs_enet.best_params_[\"elasticnet__alpha\"]:.4f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('CV MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77096585",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 14 (CORE)\n",
    "\n",
    "Comment on the optimal values of `alpha` and `l1_ratio` for ElasticNet based on the plot above. How does the CV MSE of tuned Elastic Net compare to our basineline, ridge, and lasso models? How does the performance of the models compare on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2bb3e",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4649af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edaf29",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please **change 'Student 1' and 'Student 2' at the top of the notebook to include your name(s)**.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc328b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week05.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Sara Wade"
   },
   {
    "name": "Erik Hormann"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "title": "MLPy Workshop 5: Solutions"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
